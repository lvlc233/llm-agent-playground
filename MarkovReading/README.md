# 起因
1. 人类记忆通过`回放`
2. 全上下文存储会导致成本递增
3. 全上下文存储会导致模型具有某一个方向偏好
    from 数据具有二重性,即数据作为被读取的对象的同时,也作为部分指令给出
4. 其他的关于蒙特卡洛的LLM应用(包括算法)
5. 人类的"注意力机制".
    人类的注意力有限,在阅读长文章时是通过一条条阅读和回头重新读来获取信息的.
# 想法
1. 不存储任何历史记录在LLM的上下文中,即抛弃所有user,ai,tool_call...等消息
2. 每次调用完成后对整个上下文进行处理,并将处理后的结果作为下一次调用的上下文
3. 对于长文档通过切块来进行处理
4. 对文档切块后将其作为上下文作为模型`可视`的部分进行处理
5. 通过索引来确定一条文档位置
6. 模型针对一条条语句进行阅读,使用2的过程
7. 当模型产生遗忘的时候使用`索引`指向之前的文档块的内容
# 目的
根据上述的想法,写一个`马尔可夫式的文档阅读Agent`,
本质的目的是探索`1,2`的方法是否有效
